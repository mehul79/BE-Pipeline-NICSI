# ğŸ”„ Airflow ETL Pipeline

A comprehensive **ETL (Extract, Transform, Load)** pipeline built with **Apache Airflow** for automated data processing, encryption/decryption, validation, and database operations. This project demonstrates modern data engineering practices with containerized deployment and real-time data processing capabilities.

---

## ğŸ“‹ Overview

This Airflow-based ETL system:

- Processes encrypted data from multiple API sources
- Performs decryption using Java, ASP.NET, and PHP algorithms
- Validates data integrity
- Stores results in PostgreSQL

Key features:

- Automated scheduling
- Comprehensive logging
- Robust error handling

---

## ğŸ—ï¸ Architecture

### ğŸ”§ Apache Airflow (Python + Docker)

- Automated DAG scheduling and task orchestration  
- Bash operators for modular task execution  
- Logging and monitoring  
- Containerized with Docker Compose  

### ğŸŒ Flask API Server (Python + Flask)

- RESTful endpoints for data reception  
- CORS-enabled  
- Real-time data processing  
- Token-based authentication  

### ğŸ—„ï¸ Database Layer (PostgreSQL + SQLAlchemy)

- Dynamic table creation (date-based naming)  
- Structured logs with timestamps  
- Connection pooling  
- Data persistence and retrieval  

### ğŸ” Security & Encryption

- Decryption support for Java, ASP.NET, PHP  
- Token-based encryption key management  
- Integrity and validation checks  
- Secure encrypted data pipeline  

---

## ğŸš€ Features

### âœ… Core ETL Operations

- Data extraction from multiple APIs  
- Multi-language decryption  
- Data validation and integrity checks  
- Dynamic PostgreSQL table creation  
- Logging and error tracking  

### âœ… Data Processing

- Token-based encryption key management  
- External API validation (level codes)  
- CSV export & temporary file handling  
- Timezone-aware timestamp processing  

### âœ… Infrastructure

- Docker-based multi-service setup  
- Airflow UI + Scheduler  
- PostgreSQL + pgAdmin  
- Flask API server  
- Health checks and monitoring  

---

## ğŸ› ï¸ Quick Setup

### âœ… Prerequisites

- Python 3.8+  
- Docker + Docker Compose  
- PostgreSQL  

### âœ… Setup Steps

1. **Clone the Repository**

```bash
git clone <repository-url>
cd airflow_tutorial
```

2. **Start Services**

```bash
docker-compose up -d
```

3. **Access Interfaces**

- Airflow UI: [http://localhost:8080](http://localhost:8080) (user: `mehul`, pass: `mehul`)  
- Flask API: [http://localhost:5001](http://localhost:5001)  
- pgAdmin: [http://localhost:15432](http://localhost:15432) (email: `rahul2222@gmail.com`, pass: `postgres`)  
- PostgreSQL: `localhost:5434`  

4. **Initialize DAGs**

DAGs are automatically loaded from the `dags/` folder.

---

## ğŸ“ Project Structure

```
airflow_tutorial/
â”œâ”€â”€ dags/                  
â”‚   â”œâ”€â”€ final_etl.py             # Main ETL DAG  
â”‚   â”œâ”€â”€ data_migrations.py       # Migration DAG  
â”‚   â””â”€â”€ tomtom/                  # Additional DAGs  
â”œâ”€â”€ data/                  
â”‚   â”œâ”€â”€ config.py                # Config & endpoints  
â”‚   â”œâ”€â”€ database.py              # DB setup & models  
â”‚   â”œâ”€â”€ etl_m12.py               # ETL core logic  
â”‚   â”œâ”€â”€ api_utils.py             # API fetching utils  
â”‚   â”œâ”€â”€ decryption_utils.py      # Decryption functions  
â”‚   â”œâ”€â”€ validation_utils.py      # Validation logic  
â”‚   â””â”€â”€ logging_config.py        # Logging setup  
â”œâ”€â”€ docker-compose.yaml          # Multi-service orchestration  
â”œâ”€â”€ Dockerfile                   # Airflow Docker image  
â”œâ”€â”€ requirements.txt             # Python packages  
â””â”€â”€ flask_app.py                 # Flask API server  
```

---

## ğŸ”§ DAG Workflows

### ğŸ“Œ `final_etl.py` - Main ETL Pipeline

1. **fetch_the_api** â€“ Fetch external API data  
2. **validate_the_api** â€“ Validate API data  
3. **log_the_data** â€“ Configure logging  
4. **decrypt_the_data** â€“ Decrypt based on algorithm  
5. **validate_the_data** â€“ Final data validation  
6. **final_etl_processing** â€“ Store processed data  

### ğŸ“Œ `data_migrations.py` - Migration Workflow

1. **table_created** â€“ Create PostgreSQL tables dynamically  
2. **fetch_the_api** â€“ Perform local DB ETL operations  

---

## ğŸ” Decryption Support

Supports automatic decryption based on `portLanguageId`:

- **Java** â†’ `decrypt_java_api()`  
- **ASP.NET** â†’ `decrypt_asp_dot_net_api()`  
- **PHP** â†’ `decrypt_php_data()`  

---

## ğŸ“Š Data Validation

Includes:

- Level code verification from external APIs  
- Field integrity checks (`ministryCode`, `projectCode`, etc.)  
- Null/empty value detection  
- Token validation  
- Granularity & hierarchy validation  

---

## ğŸ—„ï¸ Dynamic Database Schema

Example:

```sql
CREATE TABLE logs_YYYY_MM_DD (
    id SERIAL PRIMARY KEY,
    error VARCHAR,
    token VARCHAR,
    error_details TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);
```

---

## ğŸŒ API Endpoints

### ğŸ”Œ Flask Server (Port `5001`)

- `POST /recvApiData` â€“ Receive API data  
- `GET /getData` â€“ Fetch stored/encrypted data  

### ğŸ”— External APIs

- Data API: `http://localhost:3000/getData`  
- Encryption Key API: `http://localhost:3000/getEncryptionKey?token=`  
- Validation API: `http://10.23.124.59:2222/validateDataLevelCode`  

---

## âš™ï¸ Configuration

### ğŸ”‘ Environment Variables

```bash
DATABASE_URL="postgresql://postgres:postgres@10.25.53.161:5432/demo"
AIRFLOW_UID=50000
AIRFLOW_WWW_USER_USERNAME=mehul
AIRFLOW_WWW_USER_PASSWORD=mehul
```

---

## ğŸ“ About ETL, Cron Jobs, and Airflow

### ğŸ”„ What is ETL?

- **Extract**: Gather data from sources  
- **Transform**: Clean, validate, reshape data  
- **Load**: Store into target DB or warehouse  

### â° Why Not Cron Jobs?

- Lacks error tracking and conditional logic  
- Hard to monitor/scale/debug

### ğŸŒ€ Why Apache Airflow?

- Visual DAGs and workflow management  
- Built-in logging, retries, alerts  
- Parallel execution and scheduling  
- Web UI for monitoring  
- Modular, scalable, and production-grade  

---

## ğŸ¤ Contributing

We welcome contributions including:

- ğŸ› Bug reports & feature requests  
- âš¡ Code improvements  
- ğŸ§ª Tests & validation  
- ğŸ“ Documentation updates  
- ğŸš€ Performance optimizations  

Submit PRs or open issues to contribute!

---

## ğŸ‘¨â€ğŸ’» Author Notes

Created by **Mehul Gupta** to demonstrate a modern, production-ready ETL pipeline with Airflow, Docker, Flask, and PostgreSQL. Includes secure multi-language decryption, robust validation, and real-time processing.

---

## ğŸ“„ License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for more details.

---

**Happy data processing! ğŸ”„**