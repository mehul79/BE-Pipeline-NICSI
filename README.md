# ğŸ”„ Airflow ETL Pipeline

A comprehensive ETL (Extract, Transform, Load) pipeline built with Apache Airflow for automated data processing, encryption/decryption, validation, and database operations. This project demonstrates modern data engineering practices with containerized deployment and real-time data processing capabilities.

ğŸ“‹ Overview
This Airflow-based ETL system processes encrypted data from multiple API sources, performs decryption using various algorithms (Java, ASP.NET, PHP), validates data integrity, and stores results in PostgreSQL databases. The system features automated scheduling, comprehensive logging, and robust error handling.

ğŸ—ï¸ Architecture
ğŸ”§ Apache Airflow (Python + Docker)

- Automated DAG scheduling and task orchestration
- Bash operators for modular task execution
- Comprehensive logging and monitoring
- Containerized deployment with Docker Compose

ğŸŒ Flask API Server (Python + Flask)

- RESTful API endpoints for data reception
- CORS-enabled cross-origin requests
- Real-time data processing and encryption key management
- Token-based authentication and validation

ğŸ—„ï¸ Database Layer (PostgreSQL + SQLAlchemy)

- Dynamic table creation with date-based naming
- Structured logging with timestamps
- Connection pooling and session management
- Data persistence and retrieval operations

ğŸ” Security & Encryption

- Multi-language decryption support (Java, ASP.NET, PHP)
- Token-based encryption key retrieval
- Secure data validation and integrity checks
- Encrypted data processing pipeline

ğŸš€ Features
Core ETL Operations
âœ… Automated data extraction from multiple API sources
âœ… Multi-language decryption (Java, ASP.NET, PHP)
âœ… Comprehensive data validation and integrity checks
âœ… Dynamic database table creation and management
âœ… Real-time logging and error tracking

Data Processing
âœ… Token-based encryption key management
âœ… Level code validation against external APIs
âœ… Data granularity and hierarchy validation
âœ… CSV export and temporary file handling
âœ… Timezone-aware timestamp processing

Infrastructure
âœ… Docker containerization with multi-service architecture
âœ… Apache Airflow web interface and scheduler
âœ… PostgreSQL database with pgAdmin interface
âœ… Flask API server for data reception
âœ… Comprehensive health checks and monitoring

ğŸ› ï¸ Quick Setup
Prerequisites
Python 3.8+, Docker, Docker Compose, PostgreSQL

1. Clone Repository

```bash
git clone <repository-url>
cd airflow_tutorial
```

2. Start Services

```bash
docker-compose up -d
```

3. Access Services

- Airflow Web UI: http://localhost:8080 (username: mehul, password: mehul)
- Flask API: http://localhost:5001
- pgAdmin: http://localhost:15432 (email: rahul2222@gmail.com, password: postgres)
- PostgreSQL: localhost:5434

4. Initialize DAGs
   The system will automatically load DAGs from the `dags/` directory and start processing based on the configured schedule.

ğŸ“ Project Structure

```
airflow_tutorial/
â”œâ”€â”€ dags/                          # Apache Airflow DAG definitions
â”‚   â”œâ”€â”€ final_etl.py              # Main ETL pipeline with 6-step workflow
â”‚   â”œâ”€â”€ data_migrations.py        # Database migration and setup DAG
â”‚   â””â”€â”€ tomtom/                   # Additional DAG configurations
â”œâ”€â”€ data/                         # Core ETL processing modules
â”‚   â”œâ”€â”€ config.py                 # Configuration and API endpoints
â”‚   â”œâ”€â”€ database.py               # Database models and connection setup
â”‚   â”œâ”€â”€ etl_m12.py               # Main ETL processing logic
â”‚   â”œâ”€â”€ api_utils.py             # API data fetching utilities
â”‚   â”œâ”€â”€ decryption_utils.py      # Multi-language decryption functions
â”‚   â”œâ”€â”€ validation_utils.py      # Data validation and integrity checks
â”‚   â””â”€â”€ logging_config.py        # Logging configuration setup
â”œâ”€â”€ docker-compose.yaml          # Multi-service container orchestration
â”œâ”€â”€ Dockerfile                   # Airflow container configuration
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ flask_app.py                # Flask API server for data reception
```

ğŸ”§ DAG Workflows
Main ETL Pipeline (final_etl.py)

1. **fetch_the_api** - Retrieves data from external APIs
2. **validate_the_api** - Validates API responses and data integrity
3. **log_the_data** - Configures logging and error tracking
4. **decrypt_the_data** - Decrypts data using appropriate algorithms
5. **validate_the_data** - Performs comprehensive data validation
6. **final_etl_processing** - Processes and stores final results

Data Migration Pipeline (data_migrations.py)

1. **table_created** - Creates database tables with dynamic naming
2. **fetch_the_api** - Executes local database ETL operations

ğŸ” Decryption Support
The system supports decryption for multiple programming languages:

- **Java API** - decrypt_java_api()
- **ASP.NET API** - decrypt_asp_dot_net_api()
- **PHP Data** - decrypt_php_data()

Each decryption method is automatically selected based on the portLanguageId retrieved from the encryption key API.

ğŸ“Š Data Validation
Comprehensive validation includes:

- Level code validation against external APIs
- Field integrity checks (ministryCode, projectCode, sectorCode, deptCode)
- Null/empty value detection
- Token-based authentication validation
- Data granularity verification

ğŸ—„ï¸ Database Schema
Dynamic table creation with date-based naming:

```sql
CREATE TABLE logs_YYYY-MM-DD (
    id SERIAL PRIMARY KEY,
    error VARCHAR,
    token VARCHAR,
    error_details TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);
```

ğŸŒ API Endpoints
Flask Server (Port 5001):

- `POST /recvApiData` - Receive and process API data
- `GET /getData` - Retrieve stored data and encryption keys

External APIs:

- Data API: `http://localhost:3000/getData`
- Encryption Key API: `http://localhost:3000/getEncryptionKey?token=`
- Validation API: `http://10.23.124.59:2222/validateDataLevelCode`

ğŸ”§ Configuration
Environment Variables:

```bash
DATABASE_URL="postgresql://postgres:postgres@10.25.53.161:5432/demo"
AIRFLOW_UID=50000
AIRFLOW_WWW_USER_USERNAME=mehul
AIRFLOW_WWW_USER_PASSWORD=mehul
```

ğŸ“ About ETL, Cron Jobs, and Apache Airflow

**ETL (Extract, Transform, Load)** is a data integration process that:

- **Extract**: Retrieves data from various sources (APIs, databases, files)
- **Transform**: Cleans, validates, and processes data into the desired format
- **Load**: Stores processed data into target systems (databases, data warehouses)

**Cron Jobs** are time-based task schedulers in Unix-like systems that execute commands at specified intervals. While useful for simple automation, they lack the sophisticated orchestration, monitoring, and error handling capabilities needed for complex data pipelines.

**Apache Airflow** is a modern workflow orchestration platform that:

- Provides visual DAG (Directed Acyclic Graph) representation of workflows
- Offers built-in scheduling, monitoring, and alerting
- Supports complex dependencies and conditional execution
- Includes comprehensive logging and debugging tools
- Enables parallel task execution and resource management
- Provides a web-based UI for workflow management and monitoring

This project demonstrates how Airflow can replace traditional cron jobs with a more robust, scalable, and maintainable solution for data pipeline orchestration.

ğŸ¤ Contributing
Contributions are greatly appreciated! This project welcomes:

- Bug reports and feature requests
- Code improvements and optimizations
- Documentation enhancements
- Testing and validation improvements
- Performance optimizations

Please feel free to submit pull requests or open issues to help improve this ETL pipeline.

ğŸ‘¨â€ğŸ’» Author Notes
Built by Mehul Gupta as a comprehensive ETL solution demonstrating modern data engineering practices. This project showcases the power of Apache Airflow for orchestrating complex data workflows, implementing secure data processing with multi-language decryption support, and building scalable data pipelines with containerized deployment.

Happy data processing! ğŸ”„
